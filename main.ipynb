{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going through pytorch tutorial commenting (almost) everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn #neural network\n",
    "from torch.utils.data import DataLoader #data loader for dataset\n",
    "from torchvision import datasets # built in datasets\n",
    "from torchvision.transforms import ToTensor # transforms the image to tensor (tensor is a multi-dimensional matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:01<00:00, 17916734.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 1867249.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:00<00:00, 14933567.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# FashionMNIST dataset is a dataset of Zalando's article images consisting of 60,000 training examples and 10,000 test examples\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\", #root is the location where the data is stored\n",
    "    train=True, #true for training, false for testing\n",
    "    download=True, #true to download, false if already downloaded\n",
    "    transform=ToTensor() \n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST (\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "#if we would like to load data from csv file, we can use pandas library\n",
    "#training_data = pd.read_csv('data.csv') transform=ToTensor() is not needed since it is already in tensor(matrix) form\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size) #dataloader is used to shuffle and batch the data, shuffling for randomness\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
      "Shape of y:  torch.Size([64]) torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for X,y in test_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape) #N samles, C channels(gray scale), H height, W width\n",
    "    print(\"Shape of y: \", y.shape, y.dtype) #y is the label, so shape of y = N = batch size\n",
    "    break #break to stop the loop after one iteration\n",
    "\n",
    "test_data.targets.unique() #labels of the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f'Using {device} device') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self): #initializing the neural network\n",
    "        super().__init__() #super() is used to call the parent class constructor\n",
    "        self.flatten = nn.Flatten() #flatten the tensor to 1D \n",
    "        self.linear_relu_stack = nn.Sequential ( \n",
    "            nn.Linear(28*28, 512), #input layer\n",
    "            nn.ReLU(), #activation function\n",
    "            nn.Linear(512, 512), #hidden layer  512 in, 512 out\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10) #output layer with 10 classes, 512 in, 10 out, linear activation function because we are using cross entropy loss\n",
    "        )\n",
    "        \n",
    "    def forward(self, x): #forward pass, x is the input logits(log-odds) are the output\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x) \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=NeuralNetwork().to(device) # to(device) is used to move the model to the device (cpu or gpu)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() #negative log likelihood with softmax built in\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3) #stochastic gradient descent optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer): \n",
    "    size = len(dataloader.dataset) #size of the dataset (number of samples)\n",
    "    model.train() #set the model to training mode\n",
    "    for batch, (X, y) in enumerate(dataloader): #iterate over the batches\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        pred = model(X) #make predictions\n",
    "        loss = loss_fn(pred, y) #calculate the loss and use it in backpropagation\n",
    "        \n",
    "        loss.backward() #backpropagate the loss\n",
    "        optimizer.step() #update the parameters\n",
    "        optimizer.zero_grad() #reset the gradients to zero to address the accumulation of gradients\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch+1)*len(X) #loss.item() is used to get the scalar value held in the loss\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset) #size of the dataset\n",
    "    num_batches = len(dataloader) #number of batches\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0 \n",
    "    with torch.no_grad(): #no need to calculate gradients during testing, 'with' means that the block of code is executed without calculating the gradients\n",
    "    \n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            pred = model(X)\n",
    "            \n",
    "            loss = loss_fn(pred, y) #current loss\n",
    "            test_loss += loss.item() #overall loss to report\n",
    "            \n",
    "            correctly_predicted = (pred.argmax(1) == y).type(torch.float).sum().item() #number of correctly predicted samples true/false => 1.0/0.0 => sum of 1s => extract the scalar value from the tensor\n",
    "            correct += correctly_predicted #overall number of correctly predicted samples\n",
    "            \n",
    "    test_loss /= num_batches #average loss across all batches\n",
    "    correct /= size #accuracy\n",
    "        \n",
    "    print (f\"Test accuracy: {correct}, Average loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "loss: 2.293887  [   64/60000]\n",
      "loss: 2.284237  [ 6464/60000]\n",
      "loss: 2.265129  [12864/60000]\n",
      "loss: 2.268166  [19264/60000]\n",
      "loss: 2.242955  [25664/60000]\n",
      "loss: 2.216816  [32064/60000]\n",
      "loss: 2.222272  [38464/60000]\n",
      "loss: 2.183035  [44864/60000]\n",
      "loss: 2.177886  [51264/60000]\n",
      "loss: 2.155577  [57664/60000]\n",
      "Test accuracy: 0.447, Average loss: 2.1467758849927576\n",
      "Done\n",
      "Epoch 1\n",
      "loss: 2.157333  [   64/60000]\n",
      "loss: 2.148030  [ 6464/60000]\n",
      "loss: 2.085177  [12864/60000]\n",
      "loss: 2.108140  [19264/60000]\n",
      "loss: 2.060771  [25664/60000]\n",
      "loss: 1.999161  [32064/60000]\n",
      "loss: 2.027446  [38464/60000]\n",
      "loss: 1.940122  [44864/60000]\n",
      "loss: 1.941845  [51264/60000]\n",
      "loss: 1.885275  [57664/60000]\n",
      "Test accuracy: 0.5939, Average loss: 1.8767663825089764\n",
      "Done\n",
      "Epoch 2\n",
      "loss: 1.913813  [   64/60000]\n",
      "loss: 1.882994  [ 6464/60000]\n",
      "loss: 1.756091  [12864/60000]\n",
      "loss: 1.802019  [19264/60000]\n",
      "loss: 1.699396  [25664/60000]\n",
      "loss: 1.654137  [32064/60000]\n",
      "loss: 1.672935  [38464/60000]\n",
      "loss: 1.564328  [44864/60000]\n",
      "loss: 1.590086  [51264/60000]\n",
      "loss: 1.495192  [57664/60000]\n",
      "Test accuracy: 0.6057, Average loss: 1.5077728040658744\n",
      "Done\n",
      "Epoch 3\n",
      "loss: 1.582840  [   64/60000]\n",
      "loss: 1.545051  [ 6464/60000]\n",
      "loss: 1.382223  [12864/60000]\n",
      "loss: 1.461478  [19264/60000]\n",
      "loss: 1.346675  [25664/60000]\n",
      "loss: 1.350188  [32064/60000]\n",
      "loss: 1.358954  [38464/60000]\n",
      "loss: 1.275026  [44864/60000]\n",
      "loss: 1.316682  [51264/60000]\n",
      "loss: 1.217563  [57664/60000]\n",
      "Test accuracy: 0.6223, Average loss: 1.2450506846616223\n",
      "Done\n",
      "Epoch 4\n",
      "loss: 1.332462  [   64/60000]\n",
      "loss: 1.309969  [ 6464/60000]\n",
      "loss: 1.132550  [12864/60000]\n",
      "loss: 1.245840  [19264/60000]\n",
      "loss: 1.125497  [25664/60000]\n",
      "loss: 1.158659  [32064/60000]\n",
      "loss: 1.170131  [38464/60000]\n",
      "loss: 1.099175  [44864/60000]\n",
      "loss: 1.148415  [51264/60000]\n",
      "loss: 1.058181  [57664/60000]\n",
      "Test accuracy: 0.6376, Average loss: 1.0847739660815827\n",
      "Done\n",
      "Epoch 5\n",
      "loss: 1.165099  [   64/60000]\n",
      "loss: 1.163324  [ 6464/60000]\n",
      "loss: 0.969040  [12864/60000]\n",
      "loss: 1.113581  [19264/60000]\n",
      "loss: 0.990643  [25664/60000]\n",
      "loss: 1.029929  [32064/60000]\n",
      "loss: 1.053935  [38464/60000]\n",
      "loss: 0.986998  [44864/60000]\n",
      "loss: 1.038588  [51264/60000]\n",
      "loss: 0.959069  [57664/60000]\n",
      "Test accuracy: 0.6536, Average loss: 0.9812267232852377\n",
      "Done\n",
      "Epoch 6\n",
      "loss: 1.047150  [   64/60000]\n",
      "loss: 1.066815  [ 6464/60000]\n",
      "loss: 0.856225  [12864/60000]\n",
      "loss: 1.025296  [19264/60000]\n",
      "loss: 0.905084  [25664/60000]\n",
      "loss: 0.937175  [32064/60000]\n",
      "loss: 0.977026  [38464/60000]\n",
      "loss: 0.912576  [44864/60000]\n",
      "loss: 0.961425  [51264/60000]\n",
      "loss: 0.892031  [57664/60000]\n",
      "Test accuracy: 0.6682, Average loss: 0.9096379310462126\n",
      "Done\n",
      "Epoch 7\n",
      "loss: 0.958877  [   64/60000]\n",
      "loss: 0.998127  [ 6464/60000]\n",
      "loss: 0.774484  [12864/60000]\n",
      "loss: 0.962008  [19264/60000]\n",
      "loss: 0.846978  [25664/60000]\n",
      "loss: 0.867233  [32064/60000]\n",
      "loss: 0.922004  [38464/60000]\n",
      "loss: 0.861796  [44864/60000]\n",
      "loss: 0.904653  [51264/60000]\n",
      "loss: 0.843053  [57664/60000]\n",
      "Test accuracy: 0.6805, Average loss: 0.8571420528326824\n",
      "Done\n",
      "Epoch 8\n",
      "loss: 0.889931  [   64/60000]\n",
      "loss: 0.945638  [ 6464/60000]\n",
      "loss: 0.712831  [12864/60000]\n",
      "loss: 0.914181  [19264/60000]\n",
      "loss: 0.804835  [25664/60000]\n",
      "loss: 0.813381  [32064/60000]\n",
      "loss: 0.879869  [38464/60000]\n",
      "loss: 0.825464  [44864/60000]\n",
      "loss: 0.861221  [51264/60000]\n",
      "loss: 0.805193  [57664/60000]\n",
      "Test accuracy: 0.6965, Average loss: 0.8167562340475192\n",
      "Done\n",
      "Epoch 9\n",
      "loss: 0.834208  [   64/60000]\n",
      "loss: 0.903174  [ 6464/60000]\n",
      "loss: 0.664759  [12864/60000]\n",
      "loss: 0.876698  [19264/60000]\n",
      "loss: 0.772517  [25664/60000]\n",
      "loss: 0.771480  [32064/60000]\n",
      "loss: 0.845476  [38464/60000]\n",
      "loss: 0.798274  [44864/60000]\n",
      "loss: 0.826881  [51264/60000]\n",
      "loss: 0.774693  [57664/60000]\n",
      "Test accuracy: 0.7118, Average loss: 0.7843505113747469\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    print (f\"Epoch {i}\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "    print (f\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"7\", Actual: \"9\"\n",
      "Predicted: \"4\", Actual: \"4\"\n",
      "Predicted: \"4\", Actual: \"6\"\n",
      "Predicted: \"2\", Actual: \"6\"\n",
      "Predicted: \"0\", Actual: \"0\"\n",
      "Predicted: \"9\", Actual: \"9\"\n",
      "Predicted: \"3\", Actual: \"3\"\n",
      "Predicted: \"1\", Actual: \"1\"\n",
      "Predicted: \"8\", Actual: \"8\"\n",
      "Predicted: \"0\", Actual: \"0\"\n",
      "Predicted: \"3\", Actual: \"3\"\n",
      "Predicted: \"4\", Actual: \"4\"\n",
      "Predicted: \"2\", Actual: \"2\"\n",
      "Predicted: \"0\", Actual: \"0\"\n",
      "Predicted: \"1\", Actual: \"1\"\n",
      "Predicted: \"3\", Actual: \"3\"\n",
      "Predicted: \"7\", Actual: \"7\"\n",
      "Predicted: \"7\", Actual: \"9\"\n",
      "Predicted: \"7\", Actual: \"7\"\n",
      "Predicted: \"2\", Actual: \"2\"\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(20):\n",
    "    model.eval()\n",
    "    sample = random.randint(0, len(test_data)) #randomly select a sample from the test data\n",
    "    X, y = test_data[sample][0], test_data[sample][1] \n",
    "    with torch.no_grad():\n",
    "        pred = model(X) \n",
    "        predicted, actual = pred[0].argmax(0), y \n",
    "        print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mykyta\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\") # to save \n",
    "\n",
    "model = NeuralNetwork().to(device)                                # to load \n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True)) # to load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reimport so can run separately\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "data = [[1, 2],[3, 4]] #vanilla python  matrix (list of lists)\n",
    "x_data = torch.tensor(data) #convert list to tensor\n",
    "x_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array = np.array(data) \n",
    "x_np = torch.from_numpy(np_array) # torch.tensor() works, but torch.from_numpy() is more efficient = shares memory with numpy array\n",
    "x_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
